## Description
This table gives details about the each parameter in helmchart/qat/values.yaml file 


| Parameters                      |                               Default Value                              | Description                                                                                               |   |   |
|---------------------------------|:------------------------------------------------------------------------:|-----------------------------------------------------------------------------------------------------------|---|---|
| image:                          |                     Values related to   docker images                    |                                                                                                           |   |   |
|     baseimage                   |                             openvino/ubuntu20_dev                        | Image name should be from docker hub openvino_dev                                              |   |   |
|     ovmsserver                  |                           openvino/model_server                          | Should be default                                                                                         |   |   |
|     onnxinferenceimage          |                 openvino/onnxruntime_ep_ubuntu18:2022.1.0                | Should be default                                                                                         |   |   |
|     imagePullSecrets            |                             image-repo-secret                            | If using private   registry, please create a secret and edit this name                                    |   |   |
| mountpath                       |                         Volume mounts for   pods                         | Please edit the  <current_working_gitfolder> to your   current directory when the git is cloned           |   |   |
|     trainingvolume              |          <current_working_gitfolder>/quantization_aware_training         | Training folder   mounted  to Training pod at   /home/training                                            |   |   |
|     inferencevolume             |           <current_working_gitfolder>/openvino_optimum_inference         | Openvino-optimum   inference folder  mounted to the   inference pod at /home/inference                    |   |   |
|     onnxinferencevolume         |             <current_working_gitfolder>/onnxruntime_inference            | onnxruntime-inference   folder mounted to onnxruntime inference pod at /home/inference                    |   |   |
|     ovmsmodel                   |  <current_working_gitfolder>/quantization_aware_training/model/bert_int8 | model path as an   input to ovms server pod                                                               |   |   |
| nodeselector                    |                         Node selection for   pod                         |                                                                                                           |   |   |
|     trainingnode                |                                <train_node>                              | Please edit the   <train_node> .Nodename of the system where you want to schedule the   training          |   |   |
|     inferencenode               |                              <inference_node>                            | Please edit the   <inference_node> . Nodename of the system where you want to schedule   the inference    |   |   |
| trainingenv                     |              Environment variables   related to training pod             |                                                                                                           |   |   |
|     TRAINING_FILE               |                "/home/training/training_scripts/run_qa.py"               | Keep it defaultfor   BERT usecase. But please create a new script for a different usecase and edit   this |   |   |
|     MODEL_NAME                  |           bert-large-uncased-whole-word-masking-finetuned-squad          | Keep it defaultfor   BERT usecase.Name of the huggingface model                                           |   |   |
|     DATASET_NAME                |                                   squad                                  | Keep it defaultfor   BERT usecase. But please change the dataset for different usecase                    |   |   |
|     DO_TRAIN                    |                                    True                                  | Pass 'True' to enable   the training.                                                                     |   |   |
|     MAX_SEQ_LENGTH              |                                    256                                   | Maximum length   accepted by the inputs                                                                   |   |   |
|     MAX_TRAIN_SAMPLES           |                                    50                                    | Pass it to use a   subset of the dataset during training                                                  |   |   |
|     MAX_EVAL_SAMPLES            |                                    50                                    | Pass it to use a   subset of the dataset during training                                                  |   |   |
|     PER_DEVICE_TRAIN_BATCH_SIZE |                                     4                                    |  Input batch size for training                                                                            |   |   |
|     DOC_STRIDE                  |                                    128                                   |                                                                                                           |   |   |
|     NUM_TRAIN_EPOCHS            |                                     1                                    |  Number of epochs for training                                                                            |   |   |
|   DO_EVAL                       |                                   True                                   | Set 'True' to enable   evaluation                                                                         |   |   |
|     LEARNING_RATE               |                                 3.00E-05                                 |                                                                                                           |   |   |
|     OUTPUT_DIR                  |                       /home/training/model/bert_int8                     | directory where the   output model is generated                                                           |   |   |
|     OVERWRITE_OUTPUT_DIR        |                                   True                                   |                                                                                                           |   |   |
| inferenceenv                    |         Environment variables   related to optimum inference pod         |                                                                                                           |   |   |
|     INFERENCE_SCRIPT            |                /home/inference/inference_scripts/bert_qa.py              | Keep it defaultfor   BERT usecase. For a new usecase, please create a new script and update the   field   |   |   |
|     MODEL_TYPE                  |                                     ov                                   | MODEL_TYPE= pt for   Pytorch file.  MODEL_TYPE= ov for IR   file                                          |   |   |
|     MODEL_NAME                  |           bert-large-uncased-whole-word-masking-finetuned-squad          | Keep it defaultfor   BERT usecase.Name of the huggingface model                                           |   |   |
|     MODEL_PATH                  |                       /home/training/model/bert_int8                     | Path to Int8   optimized model                                                                            |   |   |
| onnxinferenceenv                |       Environment variables   related to onnxruntime inference pod       |                                                                                                           |   |   |
|     INFERENCE_SCRIPT            |               /home/inference/bert_inference_optimum_ort_ovep.py         | Keep it defaultfor   BERT usecase. For a new usecase, please create a new script and update the   field   |   |   |
|     NITER                       |                                    10                                    | Number of iterations                                                                                      |   |   |
|     ONNX_MODEL_PATH             |                /home/training/model/bert_int8/                           | Path to folder containing Int8   optimized onnx model                                                                       |   |   |
|     PROVIDER                    |                          OpenVINOExecutionProvider                       | Provider = CPUExecutionProvider for CPUMLAS or OpenVINOExecutionProvider for OpenVINO                     |   |   |
